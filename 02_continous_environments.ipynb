{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Continuous Environments\n",
    "## Libraries used in notebook"
   ],
   "id": "8b01738a9bead2f7"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import gymnasium as gym",
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "In previous Notebook we explored the easiest variant of RL with discrete environment and a small number of states. To understand the next part we have to explain some theory we didn't cover before.\n",
    "\n",
    "### Policies\n",
    "\n",
    "When we talk about a **policy**, we are referring to the \"behaviour\" of the agent. In other words, given a state, what action should the agent take. A policy is typically denoted by the symbol $\\pi$, and can be:\n",
    "- **Deterministic**: always returns the same action for a given state ($\\pi(s) = a$)\n",
    "- **Stochastic**: returns a probability distribution over actions ($\\pi(a|s)$)\n",
    "\n",
    "In simpler, discrete environments, a policy can be represented as a table mapping states to actions. But in continuous environments with large or infinite state spaces, this becomes impossible - we need a way to **approximate the policy** instead.\n",
    "\n",
    "### Approximating the Q-function\n",
    "\n",
    "In Q-learning, the agent learns a Q-function $Q(s, a)$ that estimates the expected return of taking action $a$ in state $s$ and then following the optimal policy. In discrete cases, we used a Q-table to store this value. But in continuous environments, we can't store all possible combinations of states and actions - there are infinitely many.\n",
    "\n",
    "To handle this, we approximate the Q-function using a **function approximator**, such as a **neural network**. This network takes a state (and possibly an action) as input and outputs an estimated Q-value.\n",
    "\n",
    "Approximating the Q-function this way allows the agent to generalize across similar states - even if it has never seen a specific state before, it can make a reasonable guess based on what it has learned.\n",
    "\n",
    "### Policy Approximation\n",
    "\n",
    "Instead of learning a Q-function and deriving a policy from it, some algorithms aim to learn the policy **directly**. This is especially common in continuous action spaces, where computing $\\arg\\max_a Q(s, a)$ (finding the best action) is intractable.\n",
    "\n",
    "In **policy-based methods**, we use a neural network (called the **policy network**) to approximate $\\pi(a|s)$ - it maps states to actions (or distributions over actions). The network is trained to maximize expected reward, often using gradient-based techniques.\n",
    "\n",
    "Some algorithms (like **actor-critic methods**) combine both approaches:\n",
    "- The **actor** learns the policy (what to do),\n",
    "- The **critic** learns the value function (how good it is)."
   ],
   "id": "b32d83d4a18fdfa7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Introduction to `stable-baselines3`",
   "id": "cc734031c331e010"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Key Algorithms for Continuous Control\n",
    "- PPO – Proximal Policy Optimization\n",
    "- DDPG – Deep Deterministic Policy Gradient\n",
    "- SAC – Soft Actor-Critic"
   ],
   "id": "75d28375c596606a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training with `stable-baselines3`\n",
    "- Setting up an environment\n",
    "- Creating and training a model\n",
    "- Saving, loading, and continuing training"
   ],
   "id": "ba4a2c0247fa6679"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tuning and Customization\n",
    "- Important hyperparameters to tune (learning rate, gamma, buffer size, etc.)\n",
    "- Callbacks and logging\n",
    "- Common issues (e.g. instability, no learning)"
   ],
   "id": "265902b7589785de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluating and Visualizing Performance\n",
    "- Monitoring reward over time\n",
    "- Using evaluate_policy()\n",
    "- Rendering and recording episodes"
   ],
   "id": "19d01428c13e9a5d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
