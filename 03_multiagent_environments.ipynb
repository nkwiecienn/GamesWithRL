{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b0b63a",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b203d2",
   "metadata": {},
   "source": [
    "So far we considered single-agent environments but they have their limitations. What if we want the agent to cooperate or compete with other agent? Both [StableBaselines3](https://stable-baselines3.readthedocs.io/en/master/) and [gymnasium](https://gymnasium.farama.org/) do not natively implement such features, but we can bypass this problemn using [pettingzoo](https://pettingzoo.farama.org/) library. In this notebook we will augument training possibilities by creating multiagent environments using pettingZoo library with StableBaselines3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91faca47",
   "metadata": {},
   "source": [
    "# PettingZoo API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66ba62",
   "metadata": {},
   "source": [
    "PettingZoo is a library based on gymnasium enabling multiagent environments. It contains AEC (Agent Environment Cycle) API for environments which agents perform actions one after another and Parallel API for simultaneous actions and observations. Moreover this library features various wrappers enabling even more features. We will describe some of them below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8438a3e",
   "metadata": {},
   "source": [
    "### AEC API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a53cd9",
   "metadata": {},
   "source": [
    "AEC (Agent Environment Cycle) API allows to represent any type of game for multiagent reinforcement learning. Here agents execute actions in turns, one after another. In following example we create pettingzoo environment representing rock paper scissors game. There is no model nor policy, only two players taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66524c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T08:17:31.985975Z",
     "start_time": "2025-06-14T08:17:15.655200Z"
    }
   },
   "outputs": [],
   "source": [
    "from pettingzoo.classic import rps_v2\n",
    "\n",
    "env = rps_v2.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedc4bd",
   "metadata": {},
   "source": [
    "In games not all moves are usually available to players. In that case we need to implement action mask that gives us all available actions for given player, below simple chess example with random moves and masking. This pettingZoo environment implements observation as dict with environment observation and action mask. We will later show how to train models with masks. For now this is how we can apply mask to sample function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993395b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T08:26:40.542883Z",
     "start_time": "2025-06-14T08:26:27.108431Z"
    }
   },
   "outputs": [],
   "source": [
    "from pettingzoo.classic import chess_v6\n",
    "\n",
    "env = chess_v6.env(render_mode=\"human\")\n",
    "env.metadata['render_fps'] = 30\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # invalid action masking is optional and environment-dependent\n",
    "        if \"action_mask\" in info:\n",
    "            mask = info[\"action_mask\"]\n",
    "        elif isinstance(observation, dict) and \"action_mask\" in observation:\n",
    "            mask = observation[\"action_mask\"]\n",
    "        else:\n",
    "            mask = None\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample(mask)\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad404cd",
   "metadata": {},
   "source": [
    "### Parallel API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4866f334",
   "metadata": {},
   "source": [
    "For simultaneous actions and observations we use alternative Parallel API. We can get actions of all agents at the same time, here with sample function. Below example shows pistonball environment in which agents cooperate to move ball to other side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b953c82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T08:28:00.891512Z",
     "start_time": "2025-06-14T08:27:50.577376Z"
    }
   },
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "parallel_env = pistonball_v6.parallel_env(render_mode=\"human\")\n",
    "observations, infos = parallel_env.reset(seed=42)\n",
    "\n",
    "while parallel_env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: parallel_env.action_space(agent).sample() for agent in parallel_env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = parallel_env.step(actions)\n",
    "parallel_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f51cb7",
   "metadata": {},
   "source": [
    "### Wrapers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40084330",
   "metadata": {},
   "source": [
    "PettingZoo features some usefull wrapers. We can convert AEC environments to Parallel and other way around with ```aec_to_parallel``` and ```parallel_to_aec```. Other usefull wraper is ```TerminateIllegalWrapper``` that disallows illegal moves. For parallel environments we need to wrap them first in ```BaseParallelWraper```. More wrappers can be found on pettingZoo documentation website. Keep in mind that most of pettingZoo native environments are already wrapped with appropriate wrappers so there is no need to do this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6f30883",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T08:28:21.680616Z",
     "start_time": "2025-06-14T08:28:21.351676Z"
    }
   },
   "outputs": [],
   "source": [
    "from pettingzoo.utils.conversions import aec_to_parallel\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "env = pistonball_v6.env()\n",
    "env = aec_to_parallel(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f6b4e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T08:28:25.475416Z",
     "start_time": "2025-06-14T08:28:25.448774Z"
    }
   },
   "outputs": [],
   "source": [
    "from pettingzoo.utils import parallel_to_aec\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "env = pistonball_v6.parallel_env()\n",
    "env = parallel_to_aec(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3320c3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T08:28:29.358709Z",
     "start_time": "2025-06-14T08:28:29.343845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.utils import TerminateIllegalWrapper\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "env = tictactoe_v3.env()\n",
    "env = TerminateIllegalWrapper(env, illegal_reward=-1)\n",
    "\n",
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9485cc5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T08:28:43.897956Z",
     "start_time": "2025-06-14T08:28:33.919239Z"
    }
   },
   "outputs": [],
   "source": [
    "from pettingzoo.utils import BaseParallelWrapper\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "parallel_env = pistonball_v6.parallel_env(render_mode=\"human\")\n",
    "parallel_env = BaseParallelWrapper(parallel_env)\n",
    "\n",
    "observations, infos = parallel_env.reset()\n",
    "\n",
    "while parallel_env.agents:\n",
    "    actions = {agent: parallel_env.action_space(agent).sample(\n",
    "    ) for agent in parallel_env.agents}  # this is where you would insert your policy\n",
    "    observations, rewards, terminations, truncations, infos = parallel_env.step(\n",
    "        actions)\n",
    "\n",
    "parallel_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd730ee",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68445b64",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8429fcb6e47a03c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T08:39:23.717659Z",
     "start_time": "2025-06-14T08:39:23.709654Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from pettingzoo.sisl import waterworld_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aceb03",
   "metadata": {},
   "source": [
    "Create and wrap environemnt.\n",
    "\n",
    "- ```ss.pettingzoo_env_to_vec_env_v1``` makes environment compatibile with stable baselines3\n",
    "- ```ss.concat_vec_envs_v1``` makes n simulations at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74115a4c49c908dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-14T08:39:40.429594Z",
     "start_time": "2025-06-14T08:39:40.323574Z"
    }
   },
   "outputs": [],
   "source": [
    "env = waterworld_v4.parallel_env()\n",
    "env.reset(seed=42)\n",
    "\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "env = ss.concat_vec_envs_v1(env, 8, num_cpus=2, base_class=\"stable_baselines3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef7cfd",
   "metadata": {},
   "source": [
    "Create PPO model, you can tune these and more parameters. This model uses ```MlpPolicy``` what means using multi-layer perceptron network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4daadd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    MlpPolicy,\n",
    "    env,\n",
    "    verbose=3,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=256,\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81c589",
   "metadata": {},
   "source": [
    "Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba4bfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed8547912d94f2ebeaefe9bb25be24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1756  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/jakub/miniconda3/envs/sb3multi/lib/python3.13/site-packages/torch/autograd/graph.py:823: UserWarning: CUDA \n",
       "initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env \n",
       "variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally \n",
       "at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
       "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/jakub/miniconda3/envs/sb3multi/lib/python3.13/site-packages/torch/autograd/graph.py:823: UserWarning: CUDA \n",
       "initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env \n",
       "variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally \n",
       "at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
       "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1584         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048466474 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.8         |\n",
      "|    explained_variance   | -0.00415     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 4.3          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000366    |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 10.5         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1515       |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 64         |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00454988 |\n",
      "|    clip_fraction        | 0.0354     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.77      |\n",
      "|    explained_variance   | 0.291      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 4.28       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.000742  |\n",
      "|    std                  | 0.965      |\n",
      "|    value_loss           | 11.7       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1455         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051311515 |\n",
      "|    clip_fraction        | 0.0454       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.73        |\n",
      "|    explained_variance   | 0.36         |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 7.3          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000712    |\n",
      "|    std                  | 0.948        |\n",
      "|    value_loss           | 14.3         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1442         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 113          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055509075 |\n",
      "|    clip_fraction        | 0.0553       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | 0.394        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 6.84         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000962    |\n",
      "|    std                  | 0.933        |\n",
      "|    value_loss           | 15.7         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1430        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004556311 |\n",
      "|    clip_fraction        | 0.061       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 7.68        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    std                  | 0.922       |\n",
      "|    value_loss           | 15.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1420        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007919838 |\n",
      "|    clip_fraction        | 0.078       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.64       |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 8.38        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00157    |\n",
      "|    std                  | 0.904       |\n",
      "|    value_loss           | 14.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1420        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 184         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006140894 |\n",
      "|    clip_fraction        | 0.0731      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.6        |\n",
      "|    explained_variance   | 0.418       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.000781   |\n",
      "|    std                  | 0.883       |\n",
      "|    value_loss           | 18.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1418         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 207          |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065908213 |\n",
      "|    clip_fraction        | 0.0729       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.56        |\n",
      "|    explained_variance   | 0.437        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 9            |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    std                  | 0.872        |\n",
      "|    value_loss           | 19.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1428        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 229         |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005607011 |\n",
      "|    clip_fraction        | 0.0789      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 7.56        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00132    |\n",
      "|    std                  | 0.852       |\n",
      "|    value_loss           | 19.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10*(2**15), progress_bar=True)\n",
    "model.save('waterworld_model')\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ede86",
   "metadata": {},
   "source": [
    "Evaluate model by simulating n games and collecting rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "245d5548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:  {'pursuer_0': np.float64(37.14175783466845), 'pursuer_1': np.float64(-173.56618499680962)}\n",
      "Avg reward: -68.21221358107059\n"
     ]
    }
   ],
   "source": [
    "env = waterworld_v4.env()\n",
    "env.reset(seed=42)\n",
    "model = PPO.load('waterworld_model')\n",
    "\n",
    "rewards = {agent: 0 for agent in env.possible_agents}\n",
    "\n",
    "for i in range(10):\n",
    "    env.reset(seed=i)\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "        for a in env.agents:\n",
    "            rewards[a] += env.rewards[a]\n",
    "        if termination or truncation:\n",
    "            break\n",
    "        else:\n",
    "            act = model.predict(obs, deterministic=True)[0]\n",
    "\n",
    "        env.step(act)\n",
    "env.close()\n",
    "\n",
    "avg_reward = sum(rewards.values()) / len(rewards.values())\n",
    "print(\"Rewards: \", rewards)\n",
    "print(f\"Avg reward: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1917ecf7",
   "metadata": {},
   "source": [
    "You can visualise model by setting ```render_mode``` flag to ```\"human\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "777bd8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = waterworld_v4.env(render_mode='human')\n",
    "env.metadata['render_fps'] = 60\n",
    "env.reset(seed=42)\n",
    "model = model = PPO.load('waterworld_model')\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        break\n",
    "    else:\n",
    "        act = model.predict(obs, deterministic=True)[0]\n",
    "\n",
    "    env.step(act)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7783f38",
   "metadata": {},
   "source": [
    "# Training environments with action mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec68a00",
   "metadata": {},
   "source": [
    "Following pettingZoo documentation to train environments containing action mask in Stable Baselines3 we need to define below wrapper. Feel free to copy paste it but let's try to explain what happens here.\n",
    "\n",
    "To use MaskablePPO from ```Stable Baselines3 - Contrib``` we must wrap environment in ```ActionMasker```. Moreover we need to pass function reference to ActionMasker that takes environment and returns action mask like in example below (```mask_fn```)\n",
    "\n",
    "PettingZoo environments choose to approach this in following fasion. They create observation as dictionary with ```observation``` and ```action_mask``` and then split it in wrapper. Here is the wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "437306a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "import pettingzoo.utils\n",
    "from pettingzoo.classic import connect_four_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46f4fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pass into other gymnasium wrappers, we need to ensure that pettingzoo's wrappper\n",
    "# can also be a gymnasium Env. Thus, we subclass under gym.Env as well.\n",
    "class SB3ActionMaskWrapper(pettingzoo.utils.BaseWrapper, gym.Env):\n",
    "    \"\"\"Wrapper to allow PettingZoo environments to be used with SB3 illegal action masking.\"\"\"\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Gymnasium-like reset function which assigns obs/action spaces to be the same for each agent.\n",
    "\n",
    "        This is required as SB3 is designed for single-agent RL and doesn't expect obs/action spaces to be functions\n",
    "        \"\"\"\n",
    "        super().reset(seed, options)\n",
    "\n",
    "        # Strip the action mask out from the observation space\n",
    "        self.observation_space = super().observation_space(self.possible_agents[0])[\n",
    "            \"observation\"\n",
    "        ]\n",
    "        self.action_space = super().action_space(self.possible_agents[0])\n",
    "\n",
    "        # Return initial observation, info (PettingZoo AEC envs do not by default)\n",
    "        return self.observe(self.agent_selection), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Gymnasium-like step function, returning observation, reward, termination, truncation, info.\n",
    "\n",
    "        The observation is for the next agent (used to determine the next action), while the remaining\n",
    "        items are for the agent that just acted (used to understand what just happened).\n",
    "        \"\"\"\n",
    "        current_agent = self.agent_selection\n",
    "\n",
    "        super().step(action)\n",
    "\n",
    "        next_agent = self.agent_selection\n",
    "        return (\n",
    "            self.observe(next_agent),\n",
    "            self._cumulative_rewards[current_agent],\n",
    "            self.terminations[current_agent],\n",
    "            self.truncations[current_agent],\n",
    "            self.infos[current_agent],\n",
    "        )\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"Return only raw observation, removing action mask.\"\"\"\n",
    "        return super().observe(agent)[\"observation\"]\n",
    "\n",
    "    def action_mask(self):\n",
    "        \"\"\"Separate function used in order to access the action mask.\"\"\"\n",
    "        return super().observe(self.agent_selection)[\"action_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78e0de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env):\n",
    "    # Do whatever you'd like in this function to return the action mask\n",
    "    # for the current env. In this example, we assume the env has a\n",
    "    # helpful method we can rely on.\n",
    "    return env.action_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea448d",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c17a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = connect_four_v3.env()\n",
    "env = SB3ActionMaskWrapper(env)\n",
    "env.reset(seed=42)\n",
    "env = ActionMasker(env, mask_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dee7b868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32f41774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024b8755ca4e442e870bd5e310dc3363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.2     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 987      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 21          |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007951414 |\n",
      "|    clip_fraction        | 0.0416      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.9        |\n",
      "|    explained_variance   | -2.45       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00133    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 0.0755      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19.7        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 705         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008569875 |\n",
      "|    clip_fraction        | 0.0567      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.9        |\n",
      "|    explained_variance   | 0.0224      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0449     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 0.0127      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20.1        |\n",
      "|    ep_rew_mean          | 0.99        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 685         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010089598 |\n",
      "|    clip_fraction        | 0.0881      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | -0.441      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.00532     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 18.8        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 676         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010065762 |\n",
      "|    clip_fraction        | 0.0828      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.85       |\n",
      "|    explained_variance   | -0.958      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0444     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.00525     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 17.1        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 669         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010075117 |\n",
      "|    clip_fraction        | 0.0911      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | -0.315      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0345     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    value_loss           | 0.00317     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 16.9        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 665         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011402028 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.81       |\n",
      "|    explained_variance   | -0.357      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0277     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    value_loss           | 0.0026      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 14.9        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 660         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010663469 |\n",
      "|    clip_fraction        | 0.0937      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.0319      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    value_loss           | 0.00197     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.5        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 656         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014470482 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | -0.0936     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0521     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    value_loss           | 0.00189     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 11.8        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015216706 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.62       |\n",
      "|    explained_variance   | 0.0254      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0555     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0379     |\n",
      "|    value_loss           | 0.00141     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.6        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 646         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015475433 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0295     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 0.00103     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.55        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016106198 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0459     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0337     |\n",
      "|    value_loss           | 0.000746    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.09        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016477432 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0325     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0396     |\n",
      "|    value_loss           | 0.000515    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.03        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 633         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020140778 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0371     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    value_loss           | 0.000381    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.78        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 632         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017266545 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.858      |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0364     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    value_loss           | 0.000225    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.36        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016191576 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.051      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 0.000104    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.16        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012948139 |\n",
      "|    clip_fraction        | 0.0916      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.539      |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0377     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 5.35e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.07        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 634         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006799562 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.455      |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00904    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 2.01e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 7.05         |\n",
      "|    ep_rew_mean          | 1            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 635          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084230825 |\n",
      "|    clip_fraction        | 0.0427       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.452       |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0445      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    value_loss           | 1.33e-05     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.01        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 636         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003441878 |\n",
      "|    clip_fraction        | 0.0446      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.427      |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    value_loss           | 7.05e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10*(2**12), progress_bar=True)\n",
    "model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e019c4",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "This is example eval function from pettingZoo documentation. This function is generalized to be used both for evaluating and rendering. First it creates environment with given parameters, then it finds latest saved policy and loads it, then it plays n games collecting their scores, then prints it and optionally renders game. There is more generalized functions like this in documentation that can be used with various environments. [SB3: Action Masked PPO for Connect Four](https://pettingzoo.farama.org/tutorials/sb3/connect_four/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "726fd251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Elliot (https://github.com/elliottower)\n",
    "\"\"\"\n",
    "def eval_action_mask(env_fn, num_games=100, render_mode=None, **env_kwargs):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "    print(\n",
    "        f\"Starting evaluation vs a random agent. Trained agent will play as {env.possible_agents[1]}.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "\n",
    "    model = MaskablePPO.load(latest_policy)\n",
    "\n",
    "    scores = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    round_rewards = []\n",
    "\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "        env.action_space(env.possible_agents[0]).seed(i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            # Separate observation and action mask\n",
    "            observation, action_mask = obs.values()\n",
    "\n",
    "            if termination or truncation:\n",
    "                # If there is a winner, keep track, otherwise don't change the scores (tie)\n",
    "                if (\n",
    "                    env.rewards[env.possible_agents[0]]\n",
    "                    != env.rewards[env.possible_agents[1]]\n",
    "                ):\n",
    "                    winner = max(env.rewards, key=env.rewards.get)\n",
    "                    scores[winner] += env.rewards[\n",
    "                        winner\n",
    "                    ]  # only tracks the largest reward (winner of game)\n",
    "                # Also track negative and positive rewards (penalizes illegal moves)\n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                # List of rewards by round, for reference\n",
    "                round_rewards.append(env.rewards)\n",
    "                break\n",
    "            else:\n",
    "                if agent == env.possible_agents[0]:\n",
    "                    act = env.action_space(agent).sample(action_mask)\n",
    "                else:\n",
    "                    # Note: PettingZoo expects integer actions # TODO: change chess to cast actions to type int?\n",
    "                    act = int(\n",
    "                        model.predict(\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    # Avoid dividing by zero\n",
    "    if sum(scores.values()) == 0:\n",
    "        winrate = 0\n",
    "    else:\n",
    "        winrate = scores[env.possible_agents[1]] / sum(scores.values())\n",
    "    print(\"Rewards by round: \", round_rewards)\n",
    "    print(\"Total rewards (incl. negative rewards): \", total_rewards)\n",
    "    print(\"Winrate: \", winrate)\n",
    "    print(\"Final scores: \", scores)\n",
    "    return round_rewards, total_rewards, winrate, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e878272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation vs a random agent. Trained agent will play as player_1.\n",
      "Rewards by round:  [{'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}]\n",
      "Total rewards (incl. negative rewards):  {'player_0': -72, 'player_1': 72}\n",
      "Winrate:  0.86\n",
      "Final scores:  {'player_0': 14, 'player_1': 86}\n",
      "Starting evaluation vs a random agent. Trained agent will play as player_1.\n",
      "Rewards by round:  [{'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}]\n",
      "Total rewards (incl. negative rewards):  {'player_0': -2, 'player_1': 2}\n",
      "Winrate:  1.0\n",
      "Final scores:  {'player_0': 0, 'player_1': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}],\n",
       " {'player_0': -2, 'player_1': 2},\n",
       " 1.0,\n",
       " {'player_0': 0, 'player_1': 2})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_fn = connect_four_v3\n",
    "\n",
    "env_kwargs = {}\n",
    "\n",
    "# Evaluate 100 games against a random agent (winrate should be ~80%)\n",
    "eval_action_mask(env_fn, num_games=100, render_mode=None, **env_kwargs)\n",
    "\n",
    "# Watch two games vs a random agent\n",
    "eval_action_mask(env_fn, num_games=2, render_mode=\"human\", **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b04c5",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "We learned to use pettingZoo library to train Stable Baselines3 models on multi-agent environments predefined in pettingZoo. This configuration has it's disadvantages, We cannot train agents that have different observation or action spaces or different purpouses. That is because in reality we train the same agent from different perspectives. PettingZoo is compatibile with more RL libraries so feel free to try them! We decided on stable baselines3 because it is used widely and is easy to install. In next notebook we'll learn to create custom environments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb3multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
