{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11da219d",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81fb423",
   "metadata": {},
   "source": [
    "In this notebook we'll go through creating custom single and multi-agent environments. Will focus on key concepts and what to remember while creating them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4685b916",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11d3efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pygame\n",
    "\n",
    "import numpy as np\n",
    "from numpy import copy\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import EzPickle\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy, MlpPolicy\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "import pettingzoo\n",
    "from pettingzoo import ParallelEnv, AECEnv\n",
    "from pettingzoo.utils import wrappers\n",
    "from pettingzoo.utils.agent_selector import agent_selector\n",
    "from pettingzoo.classic import connect_four_v3\n",
    "from pettingzoo.butterfly import knights_archers_zombies_v10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ec12b",
   "metadata": {},
   "source": [
    "# Stable Baselines3 Multiagent Action Mask Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551bb58",
   "metadata": {},
   "source": [
    "We use proposed by pettingZoo documentation wrapper for multiagent environments with action masks that is compatibile with most pettingZoo environments. We will create our environment so that we can use this wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To pass into other gymnasium wrappers, we need to ensure that pettingzoo's wrappper\n",
    "# can also be a gymnasium Env. Thus, we subclass under gym.Env as well.\n",
    "class SB3ActionMaskWrapper(pettingzoo.utils.BaseWrapper, gym.Env):\n",
    "    \"\"\"Wrapper to allow PettingZoo environments to be used with SB3 illegal action masking.\"\"\"\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Gymnasium-like reset function which assigns obs/action spaces to be the same for each agent.\n",
    "\n",
    "        This is required as SB3 is designed for single-agent RL and doesn't expect obs/action spaces to be functions\n",
    "        \"\"\"\n",
    "        super().reset(seed, options)\n",
    "\n",
    "        # Strip the action mask out from the observation space\n",
    "        self.observation_space = super().observation_space(self.possible_agents[0])[\n",
    "            \"observation\"\n",
    "        ]\n",
    "        self.action_space = super().action_space(self.possible_agents[0])\n",
    "\n",
    "        # Return initial observation, info (PettingZoo AEC envs do not by default)\n",
    "        return self.observe(self.agent_selection), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Gymnasium-like step function, returning observation, reward, termination, truncation, info.\n",
    "\n",
    "        The observation is for the next agent (used to determine the next action), while the remaining\n",
    "        items are for the agent that just acted (used to understand what just happened).\n",
    "        \"\"\"\n",
    "        current_agent = self.agent_selection\n",
    "\n",
    "        super().step(action)\n",
    "\n",
    "        next_agent = self.agent_selection\n",
    "        return (\n",
    "            self.observe(next_agent),\n",
    "            self._cumulative_rewards[current_agent],\n",
    "            self.terminations[current_agent],\n",
    "            self.truncations[current_agent],\n",
    "            self.infos[current_agent],\n",
    "        )\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"Return only raw observation, removing action mask.\"\"\"\n",
    "        return super().observe(agent)[\"observation\"]\n",
    "\n",
    "    def action_mask(self):\n",
    "        \"\"\"Separate function used in order to access the action mask.\"\"\"\n",
    "        return super().observe(self.agent_selection)[\"action_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882f82b",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4df789",
   "metadata": {},
   "source": [
    "To enable action masking in Stable Baselines3 we need to use ```MaskablePPO``` from sb3-contrib. Our environment must be wrapped by ```ActionMasker``` with masking function. PettingZoo environments are built in such manner:\n",
    "\n",
    "- we inherit from ```AECEnv```\n",
    "- we define ```possible_agents``` as string array of player names\n",
    "- we create dictionary where agents are keys and action_space is value and assign this dict to ```self.action_spaces```\n",
    "- we create dictionary where agents are keys and value is dictionary with ```observation``` and ```action_mask``` as keys.\n",
    "- we override some functions:\n",
    "    - ```observe(self, agent)``` - returning dictionary of observation and action_mask for a given player\n",
    "    - ```step(self, action)``` - performing action for a current player\n",
    "    - ```reset(self, seed=None, options=None)``` - reseting environment\n",
    "    - ```observation_space(self, agent)``` - returning observation_space for given agent\n",
    "    - ```action_space(self, agent)``` - returning action_space for given agent\n",
    "\n",
    "It is worth mentioning that to work with MaskablePPO we do not define observation_space as dictionary of observation and action_mask and we can see that the wrapper above actually separates those values and assignes only observation as observation_space. Action mask will be used in function passed to ActionMasker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b2bde72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeAecEnv(AECEnv, EzPickle):\n",
    "    # metadata = {\n",
    "    #     \"name\": \"tttAec-v0\",\n",
    "    #     \"is_parallelizable\": False,\n",
    "    # }\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.board = np.zeros(9, dtype=np.int8)\n",
    "\n",
    "        self.agents = [\"p0\", \"p1\"]\n",
    "        # define possible agents as string array\n",
    "        self.possible_agents = self.agents[:]\n",
    "\n",
    "        # define action_spaces as dictionary of agets and their action spaces (all action spaces must be the same)\n",
    "        self.action_spaces = {i: spaces.Discrete(9) for i in self.agents}\n",
    "        # define observation_spaces as dictionary of agents and their dictionaries of observation_spaces and action_masks\n",
    "        self.observation_spaces = {\n",
    "            i: spaces.Dict({\n",
    "                \"observation\": spaces.Box(low=0, high=1, shape=(3, 3, 2), dtype=np.int8),\n",
    "                \"action_mask\": spaces.Box(low=0, high=1, shape=(3, 3), dtype=np.int8),\n",
    "            })\n",
    "            for i in self.agents\n",
    "        }\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"\n",
    "        Define the observation for each agent in the environment.\n",
    "        Returns a dictionary with observation and action_mask.\n",
    "        \"\"\"\n",
    "        board_vals = np.array(self.board).reshape(3, 3)\n",
    "        cur_player = self.possible_agents.index(agent)\n",
    "        opp_player = (cur_player + 1) % 2\n",
    "\n",
    "        cur_p_board = np.equal(board_vals, cur_player + 1)\n",
    "        opp_p_board = np.equal(board_vals, opp_player + 1)\n",
    "\n",
    "        observation = np.stack(\n",
    "            [cur_p_board, opp_p_board], axis=2).astype(np.int8)\n",
    "        legal_moves = self._legal_moves() if agent == self.agent_selection else []\n",
    "\n",
    "        action_mask = np.zeros(9, \"int8\")\n",
    "        for i in legal_moves:\n",
    "            action_mask[i] = 1\n",
    "\n",
    "        return {\"observation\": observation, \"action_mask\": action_mask}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Check if agent does not try to step in a terminated or truncated state\n",
    "        if (\n",
    "            self.truncations[self.agent_selection]\n",
    "            or self.terminations[self.agent_selection]\n",
    "        ):\n",
    "            print(\n",
    "                f\"Agent {self.agent_selection} tried to step in a terminated or truncated state.\")\n",
    "            return self._was_dead_step(action)\n",
    "\n",
    "        assert self.board[0:9][action] == 0, \"played illegal move.\"\n",
    "\n",
    "        piece = self.agents.index(self.agent_selection) + 1\n",
    "        self.board[action] = piece\n",
    "\n",
    "        # Swtich to next agent (player)\n",
    "        next_agent = self._agent_selector.next()\n",
    "\n",
    "        winner = self.check_for_winner()\n",
    "\n",
    "        # If there is a winner assign reward and terminations for all agents\n",
    "        if winner:\n",
    "            self.rewards[self.agent_selection] += 1\n",
    "            self.rewards[next_agent] -= 1\n",
    "            self.terminations = {i: True for i in self.agents}\n",
    "        elif not any(x == 0 for x in self.board):\n",
    "            self.terminations = {i: True for i in self.agents}\n",
    "\n",
    "        self.agent_selection = next_agent\n",
    "        # call accumulate rewards\n",
    "        self._accumulate_rewards()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.board = np.zeros(9, dtype=np.int8)\n",
    "\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {i: 0 for i in self.agents}\n",
    "        self._cumulative_rewards = {name: 0 for name in self.agents}\n",
    "        self.terminations = {i: False for i in self.agents}\n",
    "        self.truncations = {i: False for i in self.agents}\n",
    "        self.infos = {i: {} for i in self.agents}\n",
    "\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "\n",
    "        self.agent_selection = self._agent_selector.reset()\n",
    "\n",
    "    def _legal_moves(self):\n",
    "        return [i for i in range(9) if self.board[i] == 0]\n",
    "\n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "\n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]\n",
    "\n",
    "    def check_for_winner(self):\n",
    "        board = np.reshape(self.board, (3, 3))\n",
    "        for i in range(3):\n",
    "            if np.all(board[i, :] == 1) or np.all(board[:, i] == 1):\n",
    "                return True\n",
    "            if np.all(board[i, :] == 2) or np.all(board[:, i] == 2):\n",
    "                return True\n",
    "        if np.all(np.diag(board) == 1) or np.all(np.diag(np.fliplr(board)) == 1):\n",
    "            return True\n",
    "        if np.all(np.diag(board) == 2) or np.all(np.diag(np.fliplr(board)) == 2):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5514a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returning action mask for ActionMasker wrapper\n",
    "def mask_fn(env: TicTacToeAecEnv):\n",
    "    return env.action_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0db24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToeAecEnv()\n",
    "# Enforce order of agents\n",
    "env = wrappers.OrderEnforcingWrapper(env)\n",
    "# Wrap the environment to use it with SB3\n",
    "env = SB3ActionMaskWrapper(env)\n",
    "env.reset(seed=np.random.randint(0, 1000))\n",
    "# In order to use MaskablePPO we need to wrap environment with ActionMasker\n",
    "env = ActionMasker(env, mask_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb2b312",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "996fae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = MaskablePPO(\n",
    "    MaskableActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=1,\n",
    "    ent_coef=0.01,\n",
    ")\n",
    "model.set_random_seed(seed=np.random.randint(0, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeff55fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 7.47     |\n",
      "|    ep_rew_mean     | 0.89     |\n",
      "| time/              |          |\n",
      "|    fps             | 918      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.39        |\n",
      "|    ep_rew_mean          | 0.9         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 711         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010617817 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.83       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00643     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.26        |\n",
      "|    ep_rew_mean          | 0.97        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011454549 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.62       |\n",
      "|    explained_variance   | -0.104      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0472     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.0447      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.08        |\n",
      "|    ep_rew_mean          | 0.98        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011449702 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0578     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00439    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.0225      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.98        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011831124 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0541      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.034      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    value_loss           | 0.0138      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_mask.ppo_mask.MaskablePPO at 0x739300e4b250>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2324731",
   "metadata": {},
   "source": [
    "### Example game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "091add45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "\n",
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [2 0 0]]\n",
      "\n",
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [2 1 0]]\n",
      "\n",
      "[[0 1 0]\n",
      " [2 0 0]\n",
      " [2 1 0]]\n",
      "\n",
      "[[0 1 0]\n",
      " [2 0 0]\n",
      " [2 1 1]]\n",
      "\n",
      "[[0 1 2]\n",
      " [2 0 0]\n",
      " [2 1 1]]\n",
      "\n",
      "[[1 1 2]\n",
      " [2 0 0]\n",
      " [2 1 1]]\n",
      "\n",
      "[[1 1 2]\n",
      " [2 0 2]\n",
      " [2 1 1]]\n",
      "\n",
      "[[1 1 2]\n",
      " [2 1 2]\n",
      " [2 1 1]]\n",
      "Game ended. Reward: p1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset(seed=np.random.randint(0, 1000))\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # If obs is not a dictionary, assume it directly contains the observation\n",
    "    if isinstance(obs, dict):\n",
    "        observation, action_mask = obs[\"observation\"], obs[\"action_mask\"]\n",
    "    else:\n",
    "        observation = obs\n",
    "        action_mask = env.env.action_mask()  # Retrieve the action mask separately\n",
    "\n",
    "    action, _states = model.predict(\n",
    "        observation, action_masks=action_mask, deterministic=True)\n",
    "\n",
    "    obs, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "    print(env.env.board.reshape(3, 3))\n",
    "\n",
    "    done = termination or truncation\n",
    "    if done:\n",
    "        print(f\"Game ended. Reward: {env.env.agent_selection}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e5c645",
   "metadata": {},
   "source": [
    "# Interactive game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9be289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are playing as Player 0 (p0). The model is Player 1 (p1).\n",
      "Board positions are numbered as follows:\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Your turn!\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "Model's turn...\n",
      "[[0 0 0]\n",
      " [2 1 0]\n",
      " [0 0 0]]\n",
      "Your turn!\n",
      "[[0 1 0]\n",
      " [2 1 0]\n",
      " [0 0 0]]\n",
      "Model's turn...\n",
      "[[0 1 0]\n",
      " [2 1 0]\n",
      " [2 0 0]]\n",
      "Your turn!\n",
      "[[1 1 0]\n",
      " [2 1 0]\n",
      " [2 0 0]]\n",
      "Model's turn...\n",
      "[[1 1 0]\n",
      " [2 1 2]\n",
      " [2 0 0]]\n",
      "Your turn!\n",
      "[[1 1 0]\n",
      " [2 1 2]\n",
      " [2 1 0]]\n",
      "Game over!\n",
      "You win!\n"
     ]
    }
   ],
   "source": [
    "obs, info = env.reset(seed=np.random.randint(0, 1000))\n",
    "done = False\n",
    "\n",
    "print(\"You are playing as Player 0 (p0). The model is Player 1 (p1).\")\n",
    "print(\"Board positions are numbered as follows:\")\n",
    "print(np.arange(9).reshape(3, 3))  # Display board positions for reference\n",
    "print()\n",
    "print(env.env.board.reshape(3, 3), flush=True)  # Display initial board state\n",
    "\n",
    "while not done:\n",
    "    # If obs is not a dictionary, assume it directly contains the observation\n",
    "    if isinstance(obs, dict):\n",
    "        observation, action_mask = obs[\"observation\"], obs[\"action_mask\"]\n",
    "    else:\n",
    "        observation = obs\n",
    "        action_mask = env.env.action_mask()  # Retrieve the action mask separately\n",
    "\n",
    "    # Determine the current agent\n",
    "    current_agent = env.env.agent_selection\n",
    "\n",
    "    if current_agent == \"p0\":  # Human player's turn\n",
    "        print(\"Your turn!\", flush=True)\n",
    "\n",
    "        inp = input(\"Enter your move (0-8): \")\n",
    "        if inp.isdigit() and int(inp) in range(9) and action_mask[int(inp)] == 1:\n",
    "            action = int(inp)\n",
    "        else:\n",
    "            print(\"Invalid move\")\n",
    "            break\n",
    "\n",
    "    else:  # Model's turn\n",
    "        print(\"Model's turn...\", flush=True)\n",
    "        action, _states = model.predict(\n",
    "            observation, action_masks=action_mask, deterministic=True\n",
    "        )\n",
    "\n",
    "    # Take the chosen action\n",
    "    obs, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "    # Print the updated board state\n",
    "    # Display the board state after the move\n",
    "    print(env.env.board.reshape(3, 3), flush=True)\n",
    "\n",
    "    # Check if the game has ended\n",
    "    done = termination or truncation\n",
    "    if done:\n",
    "        print(\"Game over!\")\n",
    "        if reward > 0:\n",
    "            if current_agent == \"p0\":\n",
    "                print(\"You win!\")\n",
    "            else:\n",
    "                print(\"The model wins!\")\n",
    "        elif reward < 0:\n",
    "            if current_agent == \"p0\":\n",
    "                print(\"The model wins!\")\n",
    "            else:\n",
    "                print(\"You win!\")\n",
    "        else:\n",
    "            print(\"It's a draw!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0c473",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5bc358",
   "metadata": {},
   "source": [
    "Model indeed optimizes to receive reward and do it as fast as possible but in this simple example it is clearly visible that model does not develop any meaningfull tactics as blocking. The cause of it is how we bypass limitation of not having multiagent training nativelly implemented in stable baselines3. In reality the same model plays both players what makes training instable and collapses to situation in which agent plays game to receive reward as fast as possible without meaningfull tactics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
