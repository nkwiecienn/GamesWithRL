{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Algorytmy\n",
    "Tutaj narazie będzie jakiś szkic, pozapisuję sobie to, co znajdę.\n",
    "\n",
    "Ciekawe linki:\n",
    "- https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html\n",
    "- https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "- https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/5_custom_gym_env.ipynb\n",
    "- https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html\n",
    "- https://github.com/facebookresearch/BenchMARL (Multiple Agents RL)\n",
    "- https://github.com/AI4Finance-Foundation/ElegantRL\n",
    "- https://github.com/opendilab/LightZero\n",
    "\n",
    "To co jest ciekawe i to co możemy chcieć pokazać, to różnice między środowiskami:\n",
    "- Lunar Lander - ciągłe środowisko i ciągłe akcje ew. można wybrać dyskretne środowisko z gym;\n",
    "- 2048 - dyskretne środowisko i dyskretne akcje;\n",
    "- Carcassone - duże dyskretne środowisko i duże dyskretne akcje i do tego jeszcze multiagentowe.\n",
    "\n",
    "Kolejna luźna myśl:\n",
    "- można zrobić jakąś nadklasę do tych wszystkich algorytmów (ew. jeszcze przerobić je tak, żeby wszystkie dziedziczyły z gym.Env)\n",
    "- i zrobić tam automatyczną wizualizację wyników\n",
    "- i wybór algorytmu RL (coś w stylu strategii :D)"
   ],
   "id": "22f566979501e330"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
